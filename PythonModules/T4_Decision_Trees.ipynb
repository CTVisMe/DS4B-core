{"cells":[{"cell_type":"markdown","metadata":{"id":"8W2pCzDSbf-i"},"source":["# Decision Trees\n","\n","**Fall 2024 - Instructor:  Chris Volinsky**\n","\n","\n","**Original Notebooks courtesy of Prof. Foster Provost and Rubing Li**\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"X8u8aHhWbf-l"},"source":["## Some general imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuSW0WuBbf-l"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pylab as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","sns.set(style='ticks', palette='Set2')"]},{"cell_type":"markdown","metadata":{"id":"pWoPLvYXbf-m"},"source":["## Predicting who will survive the Titanic"]},{"cell_type":"markdown","metadata":{"id":"t_JA1erfbf-n"},"source":["This time we will use a clasic introductory dataset that contains demographic and traveling information for the Titanic passengers. The goal is to predict the survival of these passengers. We will only keep a few variables of interest and transform all of them to numeric variables. We will also drop some outliers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jux0umIEbf-p","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1728587164722,"user_tz":240,"elapsed":311272,"user":{"displayName":"Christopher Volinsky","userId":"09256355646661007650"}},"outputId":"bdf0f712-5b37-4209-dbd9-d2ad84eda3cc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-53d04d0d-021e-4535-8576-12ef1c562589\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-53d04d0d-021e-4535-8576-12ef1c562589\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving titanic.csv to titanic.csv\n"]}],"source":["# Read in the data\n","# Download data to your local machine from this URL:\n","# https://drive.google.com/uc?export=download&id=1mJgS2IOvkmJLrnoVNSNcIz4e07V3wFwA\n","\n","# upload it into Colab\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n"]},{"cell_type":"code","source":["# read in data into data frame \"df\" - only the features we will use\n","\n","df = pd.read_csv(\"titanic.csv\")[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\"]]\n","df.head()\n"],"metadata":{"id":"tn_B_Bple5LH"},"execution_count":null,"outputs":[]},{"source":["#replace \"survived\" with a categorical feature with values \"lived\" and \"died\"\n","\n","df[\"survived\"] = df[\"survived\"].astype(bool)\n","df[\"survived\"] = df[\"survived\"].map({1.0: \"lived\", False: \"died\"})\n","\n","# make a barplot of survived\n","\n","df[\"survived\"].value_counts().plot(kind=\"bar\")\n","plt.show()"],"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"id":"13-7yo1pwqjm"}},{"cell_type":"markdown","source":["### Exploring the Titanic Data"],"metadata":{"id":"vPPIJwN3ZPbV"}},{"cell_type":"code","source":["# look at descriptives (note: only shows numeric)\n","\n","df.describe()"],"metadata":{"id":"6VijundgCRHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It looks like there might be some missing values of some of these variables, becasue they have different counts.  Let's confirm"],"metadata":{"id":"ytIBZduCCfBN"}},{"cell_type":"code","source":["na_count = df.isna().sum()\n","\n","na_count"],"metadata":{"id":"yX6VRWZ1CdUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wow.  There are a lot of missing values for the age variable.  We would want to check and see if the missing cases are different in any way.  For starters, are they more or less likely to die?  Lets check.\n"],"metadata":{"id":"NgGULTqACusA"}},{"cell_type":"code","source":["age_missing = df['age'].isnull()\n","\n","print(\"Percent missing age\",age_missing.mean().round(4))\n","\n","survived_missing = df[age_missing]['survived'].value_counts()\n","survived_non_missing = df[~age_missing]['survived'].value_counts()\n","\n","n_missing=survived_missing.sum()\n","\n","print(\"\\nAge Missing that Survived\\n\", (survived_missing/n_missing).round(3))\n","\n","n_nonmissing=survived_non_missing.sum()\n","\n","print(\"\\nAge Non-Missing that Survived\\n\", (survived_non_missing/n_nonmissing).round(3))\n"],"metadata":{"id":"4TYH4FnMyGbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is concerning.   In a more thorough analysis of this data, we would study the missing-ness more deeply to see if the missing values are correlated with other attributes, which would potentially add bias to the results  Also, we would consider options for filling in - imputing - the data.\n","\n","However, in order to discuss the things we want to discuss, we will drop all cases with missing values, which is easy to do in python.   We will drop missing values, which helps us to get to the things we want to discuss."],"metadata":{"id":"FkGvMIWM0mvK"}},{"cell_type":"code","source":["df = df.dropna()\n","df\n","df[\"female\"] = df[\"sex\"]\n","df[\"female\"] = (df.sex == \"female\").astype(int)\n"],"metadata":{"id":"6LOoeEHzf-e4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2pJtymElbf-q"},"source":["We'd like to use data about the passengers to predict whether they will survive. Let's start by taking a look at how well some of the variables \"split\" the data according to our target."]},{"cell_type":"code","source":["# what is the *base rate* of survival?\n","\n","\n","survival = df[\"survived\"].value_counts()/df[\"survived\"].count()\n","print(survival.round(3))"],"metadata":{"id":"80ePoqKgDnOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6TR4xmWbf-q"},"outputs":[],"source":["# do fares differ from those that survived from those who died?\n","# how do you interpret this?\n","\n","sns.boxplot(x=\"survived\", y=\"fare\", width=0.4, data=df)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7fkQ7HXGbf-s"},"source":["Above we see boxplots that show the fare distribution grouped by our target variable (survival). Alternatively, let's plot the distribution of fare according to survival:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYt-cTYzbf-v"},"outputs":[],"source":["for r in [\"died\",\"lived\"]:\n","    x_max=300 # maybe shrink to 300 to avoid outliers\n","    hist = df[df.survived == r].hist('fare',bins=range(0,x_max,10))\n","    plt.title(\"survived =\" + str(r))\n","    # plt.ylim(0,300) # can use this to plot on same axes\n","    plt.xlabel(\"fare\")\n","    plt.xlim([0,x_max])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MP3n8AURbf-w"},"source":["On might conclude from this that people that paid less are less likely to survive. **What do you think is a good split point?**. How effective is this threshold? Let's quantify it!"]},{"cell_type":"markdown","metadata":{"id":"v4hWZ0NAbf-x"},"source":["### Finding the best splits\n","\n","\n","**Entropy** ($H$) and **information gain** ($IG$) are useful tools for measuring the effectiveness of a split on the values of one variable for giving information on the value of another variable. Entropy measures how random data is, information gain is a measure of the reduction in randomness after performing a split.\n","\n","We wrote Python functions to calculate entropy and information gain\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYNk3OSgbf-x"},"outputs":[],"source":["def entropy(target_column):\n","    \"\"\"\n","        computes -sum_i p_i * log_2 (p_i) for each i\n","    \"\"\"\n","    # get the counts of each target value\n","    target_counts = target_column.value_counts().astype(float).values\n","    total = target_column.count()\n","    # compute probas\n","    probas = target_counts/total\n","    # p_i * log_2 (p_i)\n","    entropy_components = probas * np.log2(probas)\n","    # return negative sum\n","    return - entropy_components.sum()\n","\n","def information_gain(df, info_column, target_column, threshold):\n","    \"\"\"\n","        computes H(target) - H(target | info > thresh) - H(target | info <= thresh)\n","    \"\"\"\n","    # split data\n","    data_above_thresh = df[df[info_column] > threshold]\n","    data_below_thresh = df[df[info_column] <= threshold]\n","    # get entropy\n","    H = entropy(df[target_column])\n","    entropy_above = entropy(data_above_thresh[target_column])\n","    entropy_below = entropy(data_below_thresh[target_column])\n","    # compute weighted average\n","    ct_above = data_above_thresh.shape[0]\n","    ct_below = data_below_thresh.shape[0]\n","    tot = float(df.shape[0])\n","    return H - entropy_above*ct_above/tot - entropy_below*ct_below/tot"]},{"cell_type":"markdown","metadata":{"id":"6_p9qO_ibf-y"},"source":["Now that we have a way of calculating $H$ and $IG$, let's test our prior guess as a good split point as a split on fare allows us to predict people who will survive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjOG65Q3bf-z"},"outputs":[],"source":["threshold = ####\n","prior_entropy = entropy(df[\"survived\"])\n","IG = information_gain(df, \"fare\", \"survived\", threshold)\n","print (\"IG of %.4f - using a threshold of %.2f - given a prior entropy of %.4f\" % (IG, threshold, prior_entropy))"]},{"cell_type":"markdown","metadata":{"id":"xw8QsElmbf-0"},"source":["How good was our guess? Let's loop through all possible splits on fare and see what is the best!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_a0XcGtbf-0","scrolled":true},"outputs":[],"source":["def best_threshold(df, info_column, target_column, criteria=information_gain):\n","    maximum_ig = 0\n","    maximum_threshold = 0\n","\n","    for thresh in df[info_column].unique():\n","        IG = criteria(df, info_column, target_column, thresh)\n","        if IG > maximum_ig:\n","            maximum_ig = IG\n","            maximum_threshold = thresh\n","\n","    return (maximum_threshold, maximum_ig)\n","\n","col = \"fare\"\n","maximum_threshold, maximum_ig = best_threshold(df, col, \"survived\")\n","\n","print (\"The best split threshold for %s is %.2f - which provides a maximum IG of %.4f.\" % (col, maximum_threshold, maximum_ig ))"]},{"cell_type":"markdown","metadata":{"id":"6QINaTo2bf-1"},"source":["Other observed features may also give us (strong) clues about survival."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDc-A3Vtbf--"},"outputs":[],"source":["# Names of different columns\n","categorical_cols = [\"pclass\", \"female\"]\n","continuous_cols = [\"age\", \"fare\"]\n","target_col = \"survived\"\n","predictor_cols = categorical_cols + continuous_cols\n","\n","# This is to plot everything in a 2x2 space\n","rows, cols = 2, 2\n","fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(7*cols, 7*rows))\n","axs = axs.flatten()\n","posn = 0\n","\n","# Plot continous features\n","for col in continuous_cols:\n","    sns.boxplot(x=target_col, y=col, data=df, ax=axs[posn])\n","    axs[posn].set_ylabel(col)\n","    axs[posn].set_title(\"\")\n","    posn += 1\n","\n","# plots for cateogrical variables\n","pctlived = df.groupby('pclass')['survived'].value_counts(normalize=True)\n","pctlived = pctlived.unstack()*100\n","pctlived.index = pctlived.index.astype(str)\n","pctlived['lived'].plot(kind='bar',ax=axs[posn])\n","axs[posn].set_ylabel(\"Percent lived\")\n","axs[posn].set_xlabel(\"Passenger Class\")\n","axs[posn].set_title(\"\")\n","posn +=1\n","\n","\n","pctlived = df.groupby('sex')['survived'].value_counts(normalize=True)\n","pctlived = pctlived.unstack()*100\n","pctlived.index = pctlived.index.astype(str)\n","pctlived['lived'].plot(kind='bar',ax=axs[posn])\n","axs[posn].set_ylabel(\"Percent Lived\")\n","axs[posn].set_xlabel(\"Sex Class\")\n","axs[posn].set_title(\"\")\n","\n","\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"gRA10IcZbf-_"},"source":["So, then ... what feature gives the most effective split?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmi72qrobf-_"},"outputs":[],"source":["def best_split(df, info_columns, target_column, criteria=information_gain):\n","    maximum_ig = 0\n","    maximum_threshold = 0\n","    maximum_column = \"\"\n","\n","    for info_column in info_columns:\n","        thresh, ig = best_threshold(df, info_column, target_column, criteria)\n","\n","        if ig > maximum_ig:\n","            maximum_ig = ig\n","            maximum_threshold = thresh\n","            maximum_column = info_column\n","\n","    return maximum_column, maximum_threshold, maximum_ig\n","\n","max_col, max_threshold, max_ig = best_split(df, predictor_cols, \"survived\")\n","\n","\n","print (\"The best column to split on is %s giving us a IG of %.4f using a thresh of %.2f\" % (max_col, max_ig, max_threshold))"]},{"cell_type":"markdown","metadata":{"id":"Ad_cAWI0bf_A"},"source":["### Using DecisionTreeClassifier"]},{"cell_type":"markdown","metadata":{"id":"owzBAy1sbf_B"},"source":["Of course, splitting the data one time sometimes isn't enough to make accurate predictions. However, we can continue to split the data recursively (\"recursive partitioning\"), building a tree-structured model that may give better results.\n","\n","Tree-structured models are one of the most popular and powerful sorts of machine learning algorithm.  They include classification or class-probability estimation trees (aka \"decision trees\"), regression trees, and many more complex models built by combining tree-structured models, such as random forests and gradient boosted models (which usually combine trees).\n","\n","What are some other ways you might consider splitting the data?"]},{"cell_type":"markdown","metadata":{"id":"D5WZt4Y7bf_C"},"source":["Rather than build a classification tree from scratch (think if you could now do this!) let's use sklearn's implementation which includes some additional functionality."]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# Define the model (tree)\n","decision_tree = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\")\n","\n","# Fit the model\n","decision_tree.fit(df[predictor_cols], df[\"survived\"])"],"metadata":{"id":"vWUcTBlzpZrJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5-agHi2Obf_D"},"source":["We now have a classification tree, let's visualize the results!"]},{"cell_type":"code","source":["from sklearn.tree import plot_tree\n","\n","plt.figure(figsize=(15,10))\n","plot_tree(decision_tree, feature_names=predictor_cols, class_names=[\"died\", \"survived\"])\n","plt.show()"],"metadata":{"id":"MtkDLgqdpyjY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whoa!  This will almost surely overfit!  We will see later how to protect against his...\n","\n","Lets make things easier by setting max_depth to 3 - then visualize the tree and explore it to see what is important"],"metadata":{"id":"DboiuU01tNaw"}},{"cell_type":"code","source":["from sklearn import tree\n","\n","decision_tree = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\")   # Look at those 2 arguments !!!\n","\n","# Train the model on our X and y!\n","decision_tree.fit(df[predictor_cols], df[\"survived\"])\n","\n","plt.figure(figsize=(15,10))\n","tree.plot_tree(decision_tree, feature_names=predictor_cols, class_names=[\"died\", \"survived\"],\n","               filled=True, impurity=False)\n","plt.show()\n"],"metadata":{"id":"blKf8WFW0v6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluating the model\n","\n","How good is this model?  Calculate the accuracy - although we know this might not be the best metric*, it is still useful!!\n","It simply means the percent of cases that are correctly predicted in the leaf nodes of the tree.\n","\n","* because it is not measured on a hold-out set\n","* also, what is the base-rate?"],"metadata":{"id":"aM9Gjyvl1HwL"}},{"cell_type":"code","source":["from sklearn import metrics\n","accuracy = metrics.accuracy_score(decision_tree.predict(df[predictor_cols]),df[\"survived\"])\n","print ( \"Accuracy = \", round(accuracy,4))"],"metadata":{"id":"b8qh4lON1OzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Buiding the best tree using training and test sets\n","\n","\n","Use the sklearn function train_test_split to create a training and test set with an 80/20 split.  Use a random_state argument to make sure you can run with the same split repeatedly"],"metadata":{"id":"aoleCQh9BVE_"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","### define X and y (as before)\n","\n","X = df[predictor_cols]\n","y = df[\"survived\"]\n","\n","### create training and test sets\n","### CHOOSE YOUR OWN RANDOM STATE\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=###)\n","\n","### fit the model on the training set (use depth=2)\n","depth=2\n","decision_tree = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n","decision_tree.fit(X_train, y_train)\n","\n","### apply the model to the test set using .predict\n","y_pred = decision_tree.predict(X_test)\n","\n","### find the accuracy using metrics.accuracy_score()\n","accuracy = metrics.accuracy_score(y_pred, y_test)\n","print(\"Accuracy:\", round(accuracy,4))"],"metadata":{"id":"LqV-offmCAxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we will use our training/test split to help us determine what is the best depth for the tree.  There are various parameters we can use to prevent overfitting:\n","\n","- `max_depth` -  how many layers to the tree\n","- `min_samples_split` - the minimum number of samples needed at a node to split (default =2)\n","- `min_samples_leaf` - the minimum number of samples that can be in a leaf node\n","- `max_leaf_nodes` - limits the number of leaf nodes in the overall tree\n","- `min_impurity_decrease` - restricts the ability of the tree to create trivial splits\n","\n","Let's explore the depth of the tree, using `max-depth`\n","\n","Using a for loop, fit trees to your training set of max_depth = [2,3,4,5,6] and report the accuracy on the test set for each.\n","\n","What is YOUR optimal depth? (this might differ based on the `random_state` chosen)\n","\n","Optional - change `random_state` and run again."],"metadata":{"id":"kqerh_L9DEa5"}},{"cell_type":"code","source":["depth_vals = [2,3,4,5,6,7,8]\n","for i in depth_vals:\n","  depth = i\n","  decision_tree = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n","  # Tell the model what is the \"training\" data and then train it\n","  decision_tree.fit(X_train, y_train)\n","\n","  y_pred = decision_tree.predict(X_test)\n","\n","  accuracy = metrics.accuracy_score(y_pred, y_test)\n","\n","  print(\"max_depth = \",i,\"Accuracy:\", round(accuracy,4))"],"metadata":{"id":"RNQkERWXDLQN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we know the optimal depth, if this were a real-world problem, before we apply it to the real world, we would re-fit our model to the FULL data set.  (This is a contrived case because there is no actual real-world application to predicting Titanic deaths ðŸ˜± ).  But lets do it anyway.\n"],"metadata":{"id":"yGa-47HrDl8X"}},{"cell_type":"code","source":["# now that we know the best max_depth, to find the best tree...apply it to the ENTIRE data set\n","\n","opt_depth = ##OPT_DEPTH_VALUE##\n","\n","tree_final = DecisionTreeClassifier(max_depth=opt_depth, criterion=\"entropy\")\n","tree_final.fit(X,y)\n","\n","\n","plt.figure(figsize=(25,20))\n","tree.plot_tree(tree_final, feature_names=predictor_cols, class_names=[\"died\", \"survived\"],filled=True, impurity=False)\n","plt.show()"],"metadata":{"id":"bSn2Gf7ADlHz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can use `.feature_importances_` applied to the tree model to get the importance of each feature.  Find the importances of the four attributes and plot as a barplot."],"metadata":{"id":"gFg6VJxuEgb7"}},{"cell_type":"code","source":["# find feature importances and plot\n","\n","# print(predictor_cols, importances.round(4))\n","\n","scores = tree_final.feature_importances_\n","names=predictor_cols\n","\n","\n","sorted_pairs = sorted(zip(scores, names), reverse=True)\n","sorted_scores, sorted_names = zip(*sorted_pairs)\n","\n","# Create the bar plot\n","plt.figure(figsize=(10, 6))  # Optional: Adjust the figure size as needed\n","plt.bar(sorted_names, sorted_scores, color='skyblue')  # You can change the color\n","\n","# Add title and labels\n","plt.title('Scores by Name')\n","plt.xlabel('Name')\n","plt.ylabel('Score')\n","\n","# Display the plot\n","plt.xticks(rotation=45)  # Rotate names to prevent overlap\n","plt.show()"],"metadata":{"id":"aHzU_li1RUMG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tayko:  Another example for you to try"],"metadata":{"id":"6TI0ZSF6FKhg"}},{"cell_type":"markdown","source":["Tayko software example [Shmueli].\n","\n","Task is to predict whether the person made a Purchase  (0,1) given 24 attributes including the source catalog the customer received (encoded in 15 \"source\" variables) and other customer attributes.  "],"metadata":{"id":"rxWEVzomGuZe"}},{"cell_type":"code","source":["# file is at url = \"https://drive.google.com/uc?export=download&id=1wo7x7PmnCJ5-79RZXJSIAa7eS8DdrX-y\"\n","\n","uploaded = files.upload()\n","\n","\n","# read in using read_csv\n","\n","df=pd.read_csv(\"Tayko.csv\")\n","df.describe()\n"],"metadata":{"id":"CpbX8pK8HKnI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now do the same exercise here that we did above, splitting 80/20 into train/test.\n","\n","We can look at a different parameter: min_leaf_vals - which determines the minimum number of observations allowed in a leaf nodes.  This will remove small nodes and protect against overfitting.\n","\n","Use the test set to determine the best value of `min_leaf_vals`."],"metadata":{"id":"55pdUyacJKzE"}},{"cell_type":"code","source":["# Build a tree using 80/20 split\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# make sure to remove [\"sequence_number\",\"Purchase\",\"Spending\"] when creating your X data\n","# target variable is \"Purchase\"\n","\n","X = df.drop([\"sequence_number\",\"Purchase\",\"Spending\"],axis=1)\n","y = df[\"Purchase\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=123)\n"],"metadata":{"id":"arDFVnQKJ9Qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# cycle through min_leaf_vals and find the one with the best accuracy on the test data\n","\n","min_leaf_vals = list(range(2,31,2))\n","\n","for ml in min_leaf_vals:\n","\n","  decision_tree = DecisionTreeClassifier(min_samples_leaf=ml, criterion=\"entropy\")\n","\n","  decision_tree.fit(X_train, y_train)\n","\n","  y_pred = decision_tree.predict(X_test)\n","\n","  accuracy = metrics.accuracy_score(y_pred, y_test)\n","\n","  print(\"min_leaf= \",ml,\"Accuracy:\", round(accuracy,3))\n"],"metadata":{"id":"6OvIHSYygrbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit your final tree on the FULL data set - with the optimized value of min_leaf_vals, and plot it.\n","\n","best_min_leaf_val = ###\n","\n","tree_final = DecisionTreeClassifier(min_samples_leaf=best_min_leaf_val, criterion=\"entropy\")\n","\n","# fit ALL data using your optimized parameter\n","tree_final.fit(X,y)\n","\n","X.names = list(X.columns)\n","\n","plt.figure(figsize=(55,40))\n","tree.plot_tree(tree_final, feature_names=X.names, class_names=[\"NoPurchase\", \"Purchase\"],filled=True, impurity=False)\n","plt.show()"],"metadata":{"id":"26H00YJsUwBX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores = tree_final.feature_importances_\n","sorted_pairs = sorted(zip(scores.round(4), X.names), reverse=True)\n","sorted_pairs\n"],"metadata":{"id":"-sVC7g5NYYIa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Do-it-Yourself Class Exercise\n","\n","For the Tayko data,\n","- optimize on the parameter `min_impurity_decrease` - use values (0,001, 0.002, ..., 0.010)\n","- use 5-fold cross-validation to fit the model  \n","- find the optimal tree using parameter \"f1\"\n"],"metadata":{"id":"_paShSB2Yqb9"}}],"metadata":{"colab":{"provenance":[{"file_id":"1jHYmN_ngjnUie_yI62rN5L5RurTfOQp_","timestamp":1726850696117}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}