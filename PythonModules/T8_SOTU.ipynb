{"cells":[{"cell_type":"markdown","metadata":{"id":"ququEzQRA37v"},"source":["# Text Mining - State of the Union Addresses\n","\n","In this notebook we will take a look at the text from recent US State of the Union addresses to demonstrate some of our text mining functions.\n","\n","We will use the Python library `BeautifulSoup` a powerful tool for scraping text from web sites.\n","\n","[This web site](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union) contains links to the text from all SOTU addresses back to George Washington!  For our analysis we will focus on the addresses from the past 50 years.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUkKQXsLGoDw"},"outputs":[],"source":["!pip install beautifulsoup4\n","!pip install nltk\n","!pip install afinn\n","!pip install spacy\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qG4cVmYLk8-B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import requests\n","import spacy\n","from bs4 import BeautifulSoup"]},{"cell_type":"markdown","metadata":{"id":"3ROi9XBOlNUe"},"source":["# Import State of the Union text\n","\n","This data comes from transcripts of all SOTU speeches leading back to George Washington."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDOmju5hHtqZ"},"outputs":[],"source":["## Import State of the Union text\n","\n","# URL of the page containing the table of speeches\n","url = \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union\"\n","\n","# Step 1: Fetch the webpage\n","response = requests.get(url)\n","webpage_content = response.content\n","\n","# Step 2: Parse the webpage content\n","soup = BeautifulSoup(webpage_content, \"html.parser\")\n","\n","# Step 3: Find the table containing speeches\n","table = soup.find(\"table\")\n","\n","# Step 4: Extract all speech links from the table\n","speech_links = []\n","\n","for a in table.find_all(\"a\", href=True):\n","    link = a['href']\n","    # Ensure correct URL format\n","    full_link = f\"{link}\"\n","    speech_links.append(full_link)\n","\n","# Function to extract speech text from each link\n","def extract_speech_text(speech_url):\n","    try:\n","        speech_response = requests.get(speech_url)\n","        speech_soup = BeautifulSoup(speech_response.content, \"html.parser\")\n","        speech_div = speech_soup.find(\"div\", class_=\"field-docs-content\")\n","\n","        # Combine all paragraphs to form the full speech\n","        speech_text = \"\"\n","        for p in speech_div.find_all(\"p\"):\n","            speech_text += p.get_text() + \"\\n\"\n","\n","        return speech_text\n","    except Exception as e:\n","        print(f\"Error fetching speech from {speech_url}: {e}\")\n","        return \"\"\n","\n","#should be 245 speeches, but we are including the radio interviews present in the table as well\n","\n","num_recent_speeches = 48 # This goes back to 1977 - Jimmy Carter\n","\n","total_text = []\n","count = 1\n","# Step 5: Iterate through each link and extract speech text\n","for link in speech_links[:num_recent_speeches]:\n","    speech_text = extract_speech_text(link)\n","    if speech_text:\n","        print(count)\n","        count += 1\n","        total_text.append(speech_text)"]},{"cell_type":"markdown","metadata":{"id":"cg3RiObGFnVA"},"source":["Want to know which speech is from which President?  [Load this file](https://drive.google.com/uc?download&id=1tavRJ1Y3gOwRpaCAZrcAtztAdZzy6qJM).  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":56},"id":"OdyYkCUAF2Ai","outputId":"60691b14-4020-4777-e3cf-702c1f662f2b"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-25529b18-c9b1-4a3f-8d2f-12e16faacf99\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-25529b18-c9b1-4a3f-8d2f-12e16faacf99\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# file upload\n","from google.colab import files\n","uploaded = files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jazu7RCZGJX9"},"outputs":[],"source":["\n","SOTU_index = pd.read_csv(\"SOTU_index.csv\")\n","SOTU_index.Year"]},{"cell_type":"markdown","metadata":{"id":"Ee4uYZrUVK5h"},"source":["#Text cleaning\n","\n","This code cleans a list of text by removing punctuation (using regexp), converting to lowercase (`.lower()`), tokenizing, removing stopwords, and lemmatizing each word to its base form for more consistent and meaningful text processing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpDzbrOoVRgR"},"outputs":[],"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download WordNet data for lemmatization\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","\n","# Initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Stopwords list\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to clean text\n","def clean_text(text_list):\n","    cleaned_texts = []\n","    for text in text_list:\n","        # Remove punctuation and lowercase the text\n","        text = re.sub(r'[^\\w\\s]', '', text.lower())\n","        # remove numbers\n","        text = re.sub(r'[\\d]', '', text)\n","\n","        # Tokenize the text - split into an array of words.\n","        words = word_tokenize(text)\n","\n","        # Remove stopwords and lemmatize each word\n","        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","\n","        # Join the words back into a single string\n","        cleaned_text = ' '.join(words)\n","        cleaned_texts.append(cleaned_text)\n","\n","    return cleaned_texts\n","\n","\n","cleaned_total_text = clean_text(total_text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDPivJ_HCLl3"},"outputs":[],"source":["# Pick a year and print the cleaned speech from that year.\n","\n","year=????\n","idx = SOTU_index[SOTU_index['Year'] == year].index[0]\n","print(SOTU_index.iloc[[idx]])\n","cleaned_total_text[idx]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7tFI-CNzMPNN"},"source":["# Basic Analysis\n","\n","The code analyzes each speech in `total_text` by calculating its length, word count, and average syllables per word, then stores these metrics in a DataFrame for easy visualization and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71,"status":"ok","timestamp":1743540248973,"user":{"displayName":"Jingyi Yang","userId":"12585963013077623731"},"user_tz":240},"id":"btqJ63O82kLG","outputId":"85d7b706-1df8-452b-ca24-ea9499d5e2d4"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package cmudict to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/cmudict.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('cmudict')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Av8sHEGcMgGE"},"outputs":[],"source":["import pandas as pd\n","from nltk.corpus import cmudict\n","from nltk import word_tokenize\n","\n","# Load the CMU Pronouncing Dictionary for syllable counting\n","d = cmudict.dict()\n","\n","# Function to count syllables in a word\n","def syllable_count(word):\n","    word = word.lower()\n","    if word in d:\n","        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word]][0]\n","    else:\n","        # If the word isn't in the CMU dictionary, return a default of 1 syllable\n","        return 1\n","\n","# Initialize lists to store analysis results\n","speech_lengths = []\n","word_counts = []\n","average_syllables_per_word = []\n","\n","# Perform analysis on each speech in total_text\n","for speech in total_text:\n","    # Calculate length in characters\n","    length = len(speech)\n","    speech_lengths.append(length)\n","\n","    # Calculate word count\n","    words = word_tokenize(speech)\n","    word_count = len(words)\n","    word_counts.append(word_count)\n","\n","    # Calculate average syllables per word\n","    syllables = [syllable_count(word) for word in words]\n","    if words:\n","        avg_syllables = sum(syllables) / len(words)\n","    else:\n","        avg_syllables = 0\n","    average_syllables_per_word.append(avg_syllables)\n","\n","df = pd.DataFrame({\n","    'Speech Number': range(1, len(total_text) + 1),\n","    'Length (Characters)': speech_lengths,\n","    'Word Count': word_counts,\n","    'Average Syllables per Word': average_syllables_per_word\n","})\n","\n","# sort the data by the Year (the original data is not sorted by year)\n","\n","df['Year']=SOTU_index['Year']\n","# plot word count in year order\n","# order the df by Year\n","df = df.sort_values(by=['Year'])\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ls1lN-wjHU8Z"},"outputs":[],"source":["# prompt: make a plot of the Word Count by time\n","\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'df' is your DataFrame with 'Speech Number' and 'Word Count' columns\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(df['Year'], df['Word Count'])\n","plt.xlabel(\"Speech Number\")\n","plt.ylabel(\"Word Count\")\n","plt.title(\"Word Count by Speech Year\")\n","divisible_by_4_years = df['Year'][df['Year'] % 4 == 0].unique()\n","\n","# Add vertical lines for each election year\n","for year in divisible_by_4_years:\n","    plt.axvline(x=year, color='red', linestyle='--', linewidth=0.8)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5mlhXMFzNVTz"},"source":["# Finding themes in the SOTU speeches"]},{"cell_type":"markdown","metadata":{"id":"kmAErPL7LZh7"},"source":["## Using TFIDF for most \"important\" words\n","\n","We will create the TF-IDF matrix of the SOTU to try and bring out the most relevant words from each speech."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IhqwSEFUNoVA"},"outputs":[],"source":["# create a TF/IDF matrix for the speeches\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create a TfidfVectorizer object\n","# the max_df is key here to avoid words like \"state, government, tax, etc that appear in many speeches\"\n","vectorizer = TfidfVectorizer(max_df=0.7,min_df=2,ngram=(1,2)) # these values are important!\n","\n","# Fit the vectorizer to the cleaned text data\n","tfidf_matrix = vectorizer.fit_transform(cleaned_total_text)\n","\n","# Get feature names (words)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Print the shape of the TF-IDF matrix\n","print(tfidf_matrix.shape)\n","\n","# You can access the TF-IDF values for each document and term\n","# For example, to get the TF-IDF values for the first document:\n","#print(tfidf_matrix[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UCZVEDlJjEO"},"outputs":[],"source":["# prompt: find the actual words that have the highest TF-IDF value for a given speech\n","\n","year = 2002\n","\n","idx = df[df['Year'] == year].index[0]\n","\n","# Get the TF-IDF values for that year's document\n","first_document_tfidf = tfidf_matrix[idx]\n","\n","topn = 20\n","\n","# Get the indices of the top 10 TF-IDF values\n","# note: tocoo() upacks the sparse matrix so you can print it.\n","top_indices = first_document_tfidf.tocoo().col[first_document_tfidf.tocoo().data.argsort()[-topn:][::-1]]\n","\n","# Get the actual words corresponding to the top indices\n","top_words = [feature_names[i] for i in top_indices]\n","\n","# Print the top words\n","print(top_words)\n"]},{"cell_type":"markdown","metadata":{"id":"NE0VgQDuM8IR"},"source":["## Topic Modelling - NMF (Non-negative Matrix Factorization)\n","\n","Helps to find common themes across speeches.  We will use NMF, a matrix decomposition method that works on similar ideas to Principle Components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wr7qnvMPW4Br"},"outputs":[],"source":["from sklearn.decomposition import NMF\n","\n","nmf_model = NMF(n_components=25, init='nndsvd', random_state=0)\n","nmf_model.fit(tfidf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9QB4QBJSk88"},"outputs":[],"source":["# Extract the topics from NMF\n","\n","topic_word_matrix = nmf_model.components_\n","document_topic_matrix = nmf_model.transform(tfidf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHlLqcVqThOV"},"outputs":[],"source":["### Print out the topics\n","\n","# Get feature names (words)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Number of top words to display per topic\n","num_top_words = 15\n","\n","# Print topics and their top words\n","for topic_idx, topic in enumerate(nmf_model.components_):\n","    print(f\"Topic {topic_idx + 1}:\")\n","    print(\" \".join([feature_names[i]\n","                    for i in topic.argsort()[:-num_top_words - 1:-1]]))\n","    print(\"-\" * 20)  # Separator between topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"692OSuDEbucP"},"outputs":[],"source":["## Print out the dominant topic for each speech\n","\n","import numpy as np\n","\n","dominant_topics = np.argmax(document_topic_matrix, axis=1)\n","\n","def get_top_words(topic_idx, n_words=15): # Returns the top n words for a given topic index\n","  top_word_indices = nmf_model.components_[topic_idx].argsort()[:-n_words - 1:-1]\n","  return [feature_names[i] for i in top_word_indices]\n","\n","for doc_idx, topic_idx in enumerate(dominant_topics):\n","  year = SOTU_index.loc[doc_idx, 'Year']\n","  print(f\"SOTU Index {doc_idx + 1}: Year {year}\")\n","  print(f\"Dominant Topic: {topic_idx + 1}\")  # Add 1 for human-readable indexing\n","  print(f\"Top Words: {', '.join(get_top_words(topic_idx))}\")\n","  print(\"-\" * 20)"]},{"cell_type":"markdown","metadata":{"id":"LgP2Bjo3ZaQN"},"source":["The methods that work with long texts like SOTU wont necessarily work so well with shorter ones like tweets...it will probably take a lot of trial and error!"]},{"cell_type":"markdown","metadata":{"id":"wVBQMEcpZ2ix"},"source":["# Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"KvQtIc5CeqIU"},"source":["We can extract all Named Entities from the document, and then summarize the entities of a particular type:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVh1xo6_Z9bj"},"outputs":[],"source":["# Extract all Named Entities - note the different types\n","\n","\n","# Load the spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Select a speech from your total_text list\n","year = 2021\n","idx = df[df['Year'] == year].index[0]\n","speech_to_analyze = total_text[idx]  # For example, the first speech\n","\n","# Apply NER to the speech\n","doc = nlp(speech_to_analyze)\n","\n","#Print the named entities\n","for ent in doc.ents:\n","    print(f\"{ent.text} - {ent.label_}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbZQhiHIacP0"},"outputs":[],"source":["# Now create a table of all of the Persons (or any other type)\n","\n","from collections import Counter\n","\n","all_persons = []\n","\n","for ent in doc.ents:\n","  if ent.label_ == \"ORG\":\n","    all_persons.append(ent.text)\n","\n","person_counts = Counter(all_persons)\n","\n","person_df = pd.DataFrame.from_dict(person_counts, orient='index', columns=['Frequency'])\n","person_df = person_df.reset_index().sort_values(by=['Frequency'], ascending=False)\n","person_df.rename(columns={'index': 'Person'}, inplace=True)\n","\n","print(person_df)"]},{"cell_type":"markdown","metadata":{"id":"E41jKFdvciYK"},"source":["# Sentiment analysis\n","\n","using `AFINN`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQM5hhQqhguy"},"outputs":[],"source":["from afinn import Afinn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","afinn = Afinn()\n","\n","# Create a dictionary to store sentiment scores indexed by year\n","sentiment_scores_by_year = {}\n","\n","# make sure to normalize the scores by the number of words!!!\n","\n","for i, speech in enumerate(cleaned_total_text):\n","    score = afinn.score(speech)\n","    num_words = len(speech.split())\n","    normalized_score = score / num_words if num_words else 0\n","\n","    # Get the year for this speech from SOTU_index\n","    year = SOTU_index.loc[i, 'Year']\n","\n","    sentiment_scores_by_year[year] = normalized_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeXigjegz-fy"},"outputs":[],"source":["\n","# Convert the dictionary to a Pandas Series for easier plotting\n","sentiment_series = pd.Series(sentiment_scores_by_year)\n","\n","# Plot sentiment scores over time\n","plt.plot(sentiment_series.index, sentiment_series.values, 'o')\n","plt.xlabel(\"Year\")\n","plt.ylabel(\"Normalized Sentiment Score\")\n","plt.title(\"Sentiment Analysis of State of the Union Addresses (AFINN)\")\n","elec_year = np.arange(1976, 2025, 4)\n","\n","for year in elec_year:\n","    plt.axvline(x=year, color='red', linestyle='--', linewidth=0.8)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PaCf0Yi0US5"},"outputs":[],"source":["# for fun, plot the most positive (or negative words):\n","from afinn import Afinn\n","import nltk\n","\n","afinn = Afinn()\n","year = 2002  # Change this to the desired year\n","\n","# Get the index of the speech for the given year\n","idx = SOTU_index[SOTU_index['Year'] == year].index[0]\n","\n","# Get the cleaned speech text for the given year\n","speech_text = cleaned_total_text[idx]\n","\n","# Tokenize the speech text into words\n","words = nltk.word_tokenize(speech_text)\n","\n","# Create a dictionary to store word scores\n","word_scores = {}\n","for word in words:\n","    score = afinn.score(word)\n","    word_scores[word] = score\n","\n","# Sort the words by score in descending order\n","pos_words = sorted(word_scores.items(), key=lambda item: item[1], reverse=True)\n","\n","neg_words = sorted(word_scores.items(), key=lambda item: item[1],reverse=False)\n","\n","# Print the top 10 most positive words\n","print(f\"Top 10 most positive words in the speech for {year}:\")\n","print(\"POSITIVE\")\n","for word, score in pos_words[:10]:\n","    print(f\"{word}: {score}\")\n","print(\"\\nNEGATIVE\")\n","for word, score in neg_words[:10]:\n","    print(f\"{word}: {score}\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1GhytaxGf6I608lV0ODvv3fDPo0FzfdW9","timestamp":1730681295872}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}