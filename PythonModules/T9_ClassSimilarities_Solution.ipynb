{"cells":[{"cell_type":"markdown","metadata":{"id":"9yC8vIBxbhdI"},"source":["### Importing Class Similarities Data\n","\n","First we will import some modules that we might need ðŸ˜‰"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfEshs0HwdMU"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n"]},{"cell_type":"markdown","source":["##Part 1: Exploring the Data\n","\n","Now lets read in the data from the ClassSimilarities_2025Spring.csv matrix. [Click here to download the data](https://drive.google.com/uc?download&id=1KHm51Dv09t9bcSaJbSn8eCBLY8ZUsdj2)"],"metadata":{"id":"9o89D34gYGZt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4n0KZnupvuG8"},"outputs":[],"source":["# read in data\n","from google.colab import files\n","uploaded = files.upload()\n"]},{"cell_type":"code","source":["sims = pd.read_csv('ClassSimilarities_2025Spring.csv')\n","\n","sims.tail()\n"],"metadata":{"id":"6Q80_dOGJMHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Uncomment the line for your class and create the `class_df` data frame\n","\n","Call your matrix \"class_df\""],"metadata":{"id":"Kfu1z3YABkt3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvGlKrJovuG9"},"outputs":[],"source":["\n","class_df = sims[sims['Section'] == 'Section 1 (MW 930am)']\n","#class_df = sims[sims['Section'] == 'Section 2 (MW 11am)']\n","#class_df = sims[sims['Section'] == 'Section 30 (W 6pm)']"]},{"cell_type":"markdown","source":["Explore the features of interest.  \n","Sort by the mean to see which ones are most (and least) popular.  Maybe even sort hightest to lowest!\n","\n","Plot barplots to visualize the distributions of each of the features.\n","\n","Suggestion - define the columns of interest as `feature_cols` and use a for loop to iterate over all of the features of interest and plot a barplot.\n","\n","Try and get the barplot to plot in order from 1 to 5 on the x-axis. (can use \"reindex\" for this)"],"metadata":{"id":"q4fUsxurYNh1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"i54nPLQtfevQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRzS0MutvuG9"},"outputs":[],"source":["feature_cols=class_df.columns.drop(['NYUID','Timestamp','Section','Name','CoffeeTeaSoda','Sleephours'])\n","\n","\n","plt.figure(figsize=(15, 10))\n","\n","mean_scores = class_df[feature_cols].mean().sort_values(ascending=False)\n","mean_scores.round(2)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0ackGE7bhdK"},"outputs":[],"source":["\n","sorted_features = mean_scores.sort_values(ascending=False).index\n","\n","plt.figure(figsize=(15,15))\n","# Iterate over each feature\n","for i, feature in enumerate(sorted_features):\n","    # Create a subplot for each feature\n","    ax = plt.subplot(6,5, i+1)\n","\n","    # Reindex the value counts to include all values 1 through 5\n","    counts = class_df[feature].value_counts().reindex(range(1, 6), fill_value=0)\n","\n","    # Plot the bar chart\n","    counts.plot(kind='bar', ax=ax)\n","\n","    # Set the x-axis range\n","    plt.xticks(range(0, 5))\n","\n","    # Set the title and\n","    avg = mean_scores[feature].round(2)\n","    lab = f\"{feature}: {avg}\"\n","    plt.xlabel('')\n","    ax.set_title(lab, fontsize=10)\n","    ax.set_ylabel('Count')\n","\n","# Adjust the layout\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","source":["Plot a heatmap of the correlations to see which features are most positively and negatively correlated.  Use the `.corr()` function in seaborn and `sns.heatmap`.  Any interesting results here?\n","\n","\n","*you can play with colorbrewer colors here!  My favorite is cmap='RdBu_r' and dont forget to set the limits to (-1,1) using `vmax` and `vmin`*"],"metadata":{"id":"NVWzk4uyYZ4o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1x--ShybhdK"},"outputs":[],"source":["# heatmap of correlations\n","\n","import seaborn as sns\n","\n","\n","# Calculate the correlation matrix\n","corr = class_df[feature_cols].corr()\n","\n","# Create a heatmap of the correlation matrix\n","plt.figure(figsize=(15, 8))\n","sns.heatmap(corr, annot=True,fmt=\".2f\", cmap='RdBu_r', cbar=True,vmin=-1,vmax=1)\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Now plot a histogram of coffee cups per week.\n","\n","How many people dont drink any coffee? ðŸ˜²"],"metadata":{"id":"Tc5OAn7cYdzB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KilJ8IGjvuG9"},"outputs":[],"source":["\n","ax = sns.histplot(class_df['CoffeeTeaSoda'], bins=10)\n","ax.set(xlabel='Caffeine Cups per Week', ylabel='Count')\n","plt.show()\n","\n","no_coffee=class_df[class_df['CoffeeTeaSoda'] == 0].shape[0]\n","print(\"number of no coffee drinkers =\",no_coffee)"]},{"cell_type":"markdown","metadata":{"id":"HT6HBKSbvuG-"},"source":["\n","## Part 2: Looking at distances between students\n","\n"]},{"cell_type":"markdown","source":["One helpful suggestions would be to set *NYUID* as the index so that you can easily extract the data for any given  NYUID."],"metadata":{"id":"6PwZLbkxGPp8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ye2bHx8qvuG-"},"outputs":[],"source":["\n","# Set 'NYUID' as the index - this allows you to call the row by the NYUID\n","class_df.set_index('NYUID', inplace=True)\n","sims.set_index('NYUID', inplace=True) # only do this if havent done before\n"]},{"cell_type":"markdown","source":["Use `sklearn.pairwise_distances`  to calculate a distance matrix \"dist_matrix\" (It may be helpful to convert the dist_matrix into a data frame using pd.DataFrame)\n","\n","Use Manhattan distance because it will be more iterpretable.\n","\n","Identify your data with your NYUID and see if you can find the three most similar students to you and the three least similar students to you"],"metadata":{"id":"5yYp6lGVYw7m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_xxD-KDvuG-"},"outputs":[],"source":["from sklearn.metrics import pairwise_distances\n","\n","# Calculate the pairwise distances\n","dist_matrix = pairwise_distances(class_df[feature_cols].values, metric='manhattan')\n","\n","# Convert the distance matrix into a DataFrame\n","dist_df = pd.DataFrame(dist_matrix, index=class_df.index, columns=class_df.index)\n","\n","# Input NYUID you would like to query - your id here!\n","input_NYUID = '#####'  # Replace 'input_value' with the actual NYUID\n"]},{"cell_type":"markdown","source":["Find the three most similar students to you and the three least similar students to you.\n","\n","Remember that the most similar student will be the student themself!  (distance = 0) so you will have to account for that.\n","\n","also you might want to check to make sure that the input_NYUID is actuall in the index or else you will get an error."],"metadata":{"id":"oJeJTFhgY7Bp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OW4gjjcbbhdL"},"outputs":[],"source":["if (input_NYUID in class_df.index):\n","\n","    # Get the distances for the input NYUID\n","    distances = dist_df.loc[input_NYUID]\n","\n","    # Get the 3 most similar students (excluding the input student itself)\n","    most_similar = distances.nsmallest(4).iloc[1:] #starting at index 1 removes the student themself as the most similar\n","\n","    # Get the 3 least similar students\n","    least_similar = distances.nlargest(3)\n","\n","    # Print the NYUID and Name of the most and least similar students\n","    print(\"Most similar students to :\", input_NYUID, class_df.loc[input_NYUID, 'Name'])\n","    for NYUID in most_similar.index:\n","        print(NYUID, class_df.loc[NYUID, 'Name'],most_similar[NYUID])\n","\n","    print(\"\\nLeast similar students to :\", input_NYUID, class_df.loc[input_NYUID, 'Name'])\n","    for NYUID in least_similar.index:\n","        print(NYUID, class_df.loc[NYUID, 'Name'],least_similar[NYUID])\n","\n","else:\n","    print('NYUID not found in the dataset')\n"]},{"cell_type":"markdown","metadata":{"id":"P1w9ICxQvuG_"},"source":["Which two people are the closest? Because we use Manhattan distance, this distance is interpreted as the total sum of the absolute differences of the 24 features.\n","\n","\n","You can look for the smallest value in the matrix, but you need to account for the fact that the diagonals are zero...\n","\n","this one is tricky, you need to use \"unravel_index\" to find the row and col in the matrix with the lowest value...(see solution)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKZorjzuvuG_"},"outputs":[],"source":["# Replace the diagonal of the distance matrix with np.inf\n","np.fill_diagonal(dist_df.values, np.inf)\n","\n","# Find the NYUIDs of the two students who are the closest\n","min_dist = np.unravel_index(np.argmin(dist_df.values), dist_df.shape)\n","mdl=list(min_dist)\n","cls=dist_df.index[mdl]\n","\n","print( class_df.loc[cls[0], 'Name'], \":\", class_df.loc[cls[1], 'Name'], dist_df.loc[cls[0], cls[1]])\n"]},{"cell_type":"markdown","source":["Which two people in class are the furthest?"],"metadata":{"id":"7wzM3bkZZBE5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdLMDcpSvuG_"},"outputs":[],"source":["# Replace the diagonal of the distance matrix with 0\n","np.fill_diagonal(dist_df.values, 0)\n","\n","# Find the NYUIDs of the two students who are the closest\n","max_dist = np.unravel_index(np.argmax(dist_df.values), dist_df.shape)\n","mdl=list(max_dist)\n","cls=dist_df.index[mdl]\n","\n","print( class_df.loc[cls[0], 'Name'], \":\", class_df.loc[cls[1], 'Name'], dist_df.loc[cls[0], cls[1]])"]},{"cell_type":"markdown","metadata":{"id":"quAZp6N5vuHA"},"source":["##Part 3: K nearest neighbors for predicting targets##\n","\n","We want to know if the preferences data has value in predicting the Coffee consumption or the Hours Sleep.\n","\n","Since we dont have a lot of data in each class, use the entire `sims` data (both classes) and split into 80/20 and fit a knn with k=5 (aka `KNeighborsRegressor(n_neighbors=5)`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2iEPf6YvuHA"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.model_selection import train_test_split\n","\n","# remove one outlier from troll in evening section :(\n","outliers = sims[sims['Coffee'] == 69].index\n","sims = sims.drop(outliers)\n","\n","# Select the features and target\n","target = 'Coffee'\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(sims[feature_cols], sims[target], test_size=0.2, random_state=11)\n","\n","# Create a KNeighborsRegressor with k=5\n","knn = KNeighborsRegressor(n_neighbors=5)\n","\n","# Fit the model to the training data\n","knn.fit(X_train, y_train)\n"]},{"cell_type":"markdown","source":["Make a scatterplot of predicted values for the test set on the x-axis and actual values on the y-axis.\n","\n","Does the prediction seem any good?"],"metadata":{"id":"Hun1NkQGZNlx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvPGL3vevuHA"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","\n","# Make predictions on the testing data\n","y_pred = knn.predict(X_test)\n","\n","# Create a scatter plot of the actual values versus the predicted values\n","plt.scatter(y_pred,y_test,alpha=0.4)\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Actual Values')\n","plt.title('Predicted vs Actual Values for Coffee Consumption')\n","\n","# Add a reference line\n","plt.plot([y_test.min() , y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Using a for-loop - find the best k via RMSE.\n","\n","(Optional - compare to a dumb model that uses just the average of the training set)"],"metadata":{"id":"0oUW_5ArZSHL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXaKncEGbhdM"},"outputs":[],"source":["from sklearn.metrics import root_mean_squared_error\n","#yikes this is not good!  But lets calcuate the best k anyway\n","\n","rmse_best = np.inf\n","for k in range(1, 21):\n","    knn = KNeighborsRegressor(n_neighbors=k)\n","    knn.fit(X_train, y_train)\n","    y_pred = knn.predict(X_test)\n","    rmse = root_mean_squared_error(y_test, y_pred)\n","    if (rmse < rmse_best):\n","      rmse_best = rmse\n","      best_k = k\n","    print('k=', k, 'RMSE:', round(rmse,3))\n","print(\"\\nBest value of k: \", best_k,\" with RMSE= \",rmse_best)"]},{"cell_type":"code","source":["# dumb_model - predict everyone with the mean of the population\n","\n","# RMSE of using just the mean to predict\n","rmse_dumb = np.sqrt(((y_test - y_train.mean())**2).mean())\n","rmse_model = root_mean_squared_error(y_test, y_pred)\n","\n","\n","print(f\"RMSE(dumb) = {round(rmse_dumb,2)} and RMSE(model) = {round(rmse_best,2)}\")\n"],"metadata":{"id":"fR0YSIN4G3sO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfuENUMibhdM"},"source":["## Part 4: Clustering\n","\n","Lets perform hierarchical clustering to find the \"10-cluster solution\" and print out the clusters.\n","\n","Did any of you end up in clusters with your project team members?\n","\n","All code provided...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4g9Kx_QbhdM"},"outputs":[],"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","# Note: this is a different way than used in the HierClust.ipynb notebook.  both are fine!\n","# Create an AgglomerativeClustering model with 6 clusters\n","cluster = AgglomerativeClustering(n_clusters=8, linkage='ward')\n","\n","# Fit the model to the data and predict the cluster labels\n","cluster_labels = cluster.fit_predict(class_df[feature_cols])\n","student_clusters = class_df.assign(Cluster=cluster_labels)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u5JrSGubhdM"},"outputs":[],"source":["\n","for cluster, data in student_clusters.groupby('Cluster'):\n","    print(f\"Cluster {cluster}:\")\n","    print(data.Name.tolist())\n","    print()"]},{"cell_type":"markdown","source":["Plot the dendrogram.\n","\n","Play around with the following  parameters:\n","in the linkage function : `method` and `metric`\n","\n","in the dendrogram: `orientation` - for the direction of the plot\n","                : `color_threshold` - for how the plot is colored showing different clusters\n","\n","can use `labels=class_df.index` to show NYUID instead of names...\n"],"metadata":{"id":"MTYpxw1dZeM_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LA_Mpez1bhdM"},"outputs":[],"source":["from scipy.cluster.hierarchy import dendrogram, linkage\n","\n","# Create a linkage matrix\n","linked = linkage(class_df[feature_cols], 'ward')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(10, 7))\n","dendrogram(linked, orientation='right', color_threshold=8, labels=class_df.Name, distance_sort='descending', show_leaf_counts=True)\n","plt.show()"]},{"cell_type":"markdown","source":["For fun - we can also cluster the *features* to see which are most similar...by passing the feature correlation matrix into the `linked` function\n","\n","\n"],"metadata":{"id":"8toMUL8Zczxz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4--AGPsbhdM"},"outputs":[],"source":["# Calculate the correlation matrix\n","corr = class_df[feature_cols].corr()\n","\n","# Create a linkage matrix based on the correlation matrix\n","linked = linkage(corr, 'ward')\n","\n","# Create a dendrogram\n","plt.figure(figsize=(10, 7))\n","dendrogram(linked, labels=corr.columns, orientation='right',color_threshold = 2)\n","plt.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1jl2JXZysGi8MJzY2zSYOilVwFEff1uzM","timestamp":1731425660592}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}