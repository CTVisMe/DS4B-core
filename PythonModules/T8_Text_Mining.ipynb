{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"1in3BbiWiVPp6-7S2-PZTFnzsvA2_zz7i","timestamp":1730594521521}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jOGCfHdvaDhY"},"source":["# Text Mining\n","\n","\n","**Data Science for Business - Instructor:  Chris Volinsky**\n","\n"]},{"cell_type":"markdown","source":["In this notebook we will be using features extracted from text as input into supervised (predictive) models."],"metadata":{"id":"G7922HtQc1QU"}},{"cell_type":"code","metadata":{"id":"XoTP4cq2aDhb"},"source":["# Import the libraries we will be using\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import matplotlib.pylab as plt\n","%matplotlib inline\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8tQAY5YaDhm"},"source":["## Application: Analyzing Customer Tweets for an Airline Company"]},{"cell_type":"markdown","metadata":{"id":"0HyNJx_saDhq"},"source":["Our problem setting: You've been hired by Trans American Airlines (TAA) as a business analytics professional. One of the top priorities of TAA is  customer service. For TAA, it is of utmost importance to identify whenever customers are unhappy with the way employees have treated them. You've been hired to analyze twitter data in order to detect whenever a customer has complaints about flight attendants. Tweets suspected to be related to flight attendant complaints should be forwarded directly to the customer service department in order to track the issue and take corrective actions.  \n","\n","Let's start by loading the training data, which has been hand labelled with the subject of the tweet and the text of the tweet itself."]},{"cell_type":"markdown","source":["[Click here](https://drive.google.com/uc?download&id=1zgbAtmg3Pm2Wg7vMWujsbT2sUBe__Qy7) to download the file \"TAA_tweets.csv\""],"metadata":{"id":"-iGjltRqgL0h"}},{"cell_type":"code","source":["#select file from computer\n","\n","from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"id":"C6PqjDHXMg4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6an9WIR1aDht"},"source":["\n","tweets = pd.read_csv(\"TAA_tweets.csv\")\n","\n","pd.set_option(\"display.max_colwidth\", 1000)\n","tweets[['negativereason','text']].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMugFtwxaDh1"},"source":["Let's take a look at what do people complain about in Twitter."]},{"cell_type":"code","metadata":{"id":"GMy-_VvOaDh3"},"source":["tweets.negativereason.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GdFVqVr1aDiA"},"source":["We will define our target variable based on \"Flight Attendant Complaints\"\n","\n"]},{"cell_type":"code","metadata":{"id":"Qxv3vGbLaDiD"},"source":["pd.set_option(\"display.max_colwidth\", 1000)  # To display up to 1000 characters\n","# We'll call our target variable \"is_fa_complaint\" and keep only the text as a \"feature\" (really, the text is the field from which we will engineer features)\n","complaint = \"Flight Attendant Complaints\"\n","is_complaint = tweets.negativereason == complaint\n","tweets['is_complaint']=is_complaint\n","tweets[is_complaint]['text']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXKTkHj3aDiN"},"source":["Let's take a look at the percentage of tweets related to complaints about flight attendants, aka the base rate:"]},{"cell_type":"code","metadata":{"id":"P8UILyBmaDiP"},"source":["is_complaint.mean().round(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhDqSSwEaDii"},"source":["X = tweets['text']\n","Y = tweets['is_complaint']\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S9EaYDXRaDit"},"source":["## Term Frequency Matrix using Binary CountVectorizer\n","How can we turn the large amount of text for each record into useful features?\n","\n","We want to create a Term Frequency (TF) matrix, which keeps track of whether or not a word appears in a document/record. The easiest TF matrix is binary - it simply has zeros and ones for which words appear in the document.\n","\n","You can do this in sklearn with a CountVectorizer() and setting binary to true. The process is very similar to how you fit a model: you will fit a CounterVectorizer(). This will figure out what words exist in your data."]},{"cell_type":"code","metadata":{"id":"lrjKfevpaDix"},"source":["binary_vectorizer = CountVectorizer(binary=True)\n","binary_vectorizer.fit(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lets look at some of the words\n","vocabulary_list = list(zip( binary_vectorizer.vocabulary_.keys(), binary_vectorizer.vocabulary_.values()) )\n","vocabulary_list[0:20]"],"metadata":{"id":"LKTecjEEjU0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFZwIwwWaDi3"},"source":["How big is the vocabulary in tweets?\n"]},{"cell_type":"code","metadata":{"id":"1CcpjAukaDi5"},"source":["vocabulary_size = len(binary_vectorizer.vocabulary_)\n","print(f\"Vocabulary size: {vocabulary_size}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QVLEmreaDjB"},"source":["Now that we have a list of the words are in the data, we can transform our text into a clean matrix. Use .transform() on the raw data using our fitted CountVectorizer(). You will do this for the training and test data. What do you think happens if there are new words in the test data that were not seen in the training data?"]},{"cell_type":"code","metadata":{"id":"SZdHAgoHaDjE"},"source":["X_train_binary = binary_vectorizer.transform(X_train)\n","X_test_binary = binary_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhsFILAhaDjL"},"source":["We can take a look at our new `X_test_binary`."]},{"cell_type":"code","metadata":{"id":"x2M7l7ERgRpP"},"source":["X_train_binary.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrFT2qgmaDjN"},"source":["X_test_binary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ia3glHapaDjU"},"source":["Sparse matrix? Where is our data?\n","\n","If you look at the output above, you will see that it is being stored in a *sparse* matrix (as opposed to the typical dense matrix) that is 3k rows long and 13k columns. The rows here are records in the original data and the columns are words. Given the shape, this means there are 39m cells that should have values. However, from the above, we can see that only 46k cells (~0.12%) of the cells have values! Why is this?\n","\n","To save space, sklearn uses a sparse matrix. This means that only values that are not zero are stored. This saves a ton of space! This also means that visualizing the data is a little trickier. Let's look at a very small chunk."]},{"cell_type":"code","metadata":{"id":"CA_LkpIKaDjW"},"source":["# Recall that 13183 is the index for \"you\"\n","X_test_binary[0:20, 13180:13200].todense()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gRRQtvk1aDjc"},"source":["\n","Now that we have a ton of features (one for every word!) let's try using a logistic regression model to predict which tweets are about flight attendant complaints.  \n","\n","We'll need some regularlization, so we will set C=1"]},{"cell_type":"code","metadata":{"id":"wOm9gaMqaDje"},"source":["LogReg_bin = LogisticRegression(solver='liblinear',C=1)\n","LogReg_bin.fit(X_train_binary, Y_train)\n","# extract probabilities\n","\n","probs = LogReg_bin.predict_proba(X_test_binary)[:,1]\n","y_pred = LogReg_bin.predict(X_test_binary)\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","\n","# print roc curve\n","plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n","plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title('ROC - Binary (area = %0.4f)' % auc)\n","print (\"AUC  = \",auc.round(3))\n","\n","\n","#Note that if you were doing this for real, you'd want to make sure you are regularizing well!\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qigohEb7aDjm"},"source":["## CountVectorizer with Counts\n","Instead of using a 0 or 1 to represent the occurence of a word, we can use the actual counts. We do this the same way as before, but now we leave `binary` set to `false` (the default value)."]},{"cell_type":"code","source":["# Fit a counter\n","ngram_stopvectorizer = CountVectorizer()\n","ngram_stopvectorizer.fit(X_train)\n","\n","X_train_counts = ngram_stopvectorizer.transform(X_train)\n","X_test_counts = ngram_stopvectorizer.transform(X_test)"],"metadata":{"id":"V7_XpUP0U0iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvaoF_F0aDjo"},"source":["# Model\n","LogReg_counts = LogisticRegression(solver='liblinear',C=1)\n","LogReg_counts.fit(X_train_counts, Y_train)\n","\n","# extract probabilities\n","probs = LogReg_counts.predict_proba(X_test_counts)[:,1]\n","y_pred = LogReg_counts.predict(X_test_counts)\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","\n","# print roc curve\n","plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n","plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title('Counts (area = %0.4f)' % auc)\n","print (\"AUC  = \",auc.round(3))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFOYPcXtaDju"},"source":["## TF-IDF Vectorizer\n","\n","Often we can improve performance by combining the term frequency in a docuemnt with the term frequency across documents (inverse document frequency - IDF). This way more important (rare) words get more weight. This is called a TF-IDF matrix.  \n","\n","Python does this via `TfidfVectorizer()`"]},{"cell_type":"code","metadata":{"id":"Clx7EIPmaDjw"},"source":["# Fit a counter\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_vectorizer.fit(X_train)\n","\n","X_train_tfidf = tfidf_vectorizer.transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_tfidf = LogisticRegression(solver='liblinear',C=1)\n","model_tfidf.fit(X_train_tfidf, Y_train)\n","\n","LogReg_tfidf = LogisticRegression(solver='liblinear',C=1)\n","LogReg_tfidf.fit(X_train_tfidf, Y_train)\n","# extract probabilities\n","\n","probs = LogReg_tfidf.predict_proba(X_test_tfidf)[:,1]\n","y_pred = LogReg_tfidf.predict(X_test_tfidf)\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","\n","# print roc curve\n","plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n","plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title('TFIDF (area = %0.4f)' % auc)\n","print (\"AUC  = \",auc.round(3))\n"],"metadata":{"id":"ncdyIi_qWpx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b8obGxAFECYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Ay1-lPBaDj4"},"source":["The `CountVectorizer()` and `TfidfVectorizer()` functions have many options.\n","\n","We discussed in class the importance of pre-processing, and some of that can be done within the Vectorizer functions.  For instance, you can remove **stopwords** which are unimportant English words.  \n","\n","You can also include. **n-grams** which are combinations of words which appear often.  **Bigrams** (n-grams with n=2) can find two word phrases that are often used and include them in the TF/IDF matrices.  Be careful, increasing n will increase the complexity of your model."]},{"cell_type":"code","metadata":{"id":"XuKviO7-aDj5"},"source":["# Removing stop words and ngrams up to n=2\n","\n","# Fit a counter\n","ngram_stop_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n","ngram_stop_vectorizer.fit(X_train)\n","\n","X_train_ngs = ngram_stop_vectorizer.transform(X_train)\n","X_test_ngs = ngram_stop_vectorizer.transform(X_test)\n","\n","LogReg_ngs = LogisticRegression(solver='liblinear',C=1)\n","LogReg_ngs.fit(X_train_ngs, Y_train)\n","\n","# extract probabilities\n","\n","probs = LogReg_ngs.predict_proba(X_test_ngs)[:,1]\n","y_pred = LogReg_ngs.predict(X_test_ngs)\n","\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","\n","# print roc curve\n","plt.plot(fpr, tpr, label='BiGrams+Stopwords (area = %0.4f)' % auc)\n","plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend()\n","plt.title('BiGrams+Stopwords (area = %0.4f)' % auc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# what n-grams are used?\n","feature_names = ngram_stop_vectorizer.get_feature_names_out()\n","feature_importance = LogReg_ngs.coef_[0]  # Assuming binary classification\n","feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n","top_bi_grams = feature_importance_df.sort_values(by='importance', ascending=False)\n","\n","N = 20  # You can change this to display more or fewer bi-grams\n","print(top_bi_grams.head(N))"],"metadata":{"id":"FEpkOEXcY2hI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This list implies that we could also do with some stemming, to combine terms like \"gate agent\" and \"gate agents\".  This can be done with the function `PorterStemmer()` in the library `nltk`."],"metadata":{"id":"dfN9S-DomajG"}},{"cell_type":"markdown","metadata":{"id":"0qopawLeaDj_"},"source":["## Naive and Multinomial Naive Bayes Models\n","\n","Naive Bayes is a class of classification models built off of the idea that all words can be modelled independent of one another.  However, it only works for binary term frequency matrices.\n","\n","Multinomial Naive Bayes (`MultinomialNB`) is an extension of Naive Bayes which works off of a CountVectorized matrix.\n","\n","Using this model in sklearn works just the same as the other classification models we've seen ([More details here..](http://scikit-learn.org/stable/modules/naive_bayes.html))\n","\n","Lets fit both of these and see which one performs better.\n"]},{"cell_type":"code","source":["from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n","\n","#Naive Bayes works on the Binary matrix\n","\n","NaiveB = BernoulliNB(alpha=1)\n","NaiveB.fit(X_train_binary, Y_train)\n","\n","probs = NaiveB.predict_proba(X_test_binary)[:,1]\n","y_pred = NaiveB.predict(X_test_binary)\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","\n","# print roc curve\n","plt.plot(fpr, tpr, label='Naive Bayes (area = %0.4f)' % auc)\n","plt.legend(\"Binary\")\n","\n","MultinomialNB = MultinomialNB(alpha=1)\n","MultinomialNB.fit(X_train_counts, Y_train)\n","\n","probs = MultinomialNB.predict_proba(X_test_counts)[:,1]\n","y_pred = MultinomialNB.predict(X_test_counts)\n","# get ROC score\n","fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","auc = metrics.roc_auc_score(Y_test, probs)\n","plt.plot(fpr, tpr, label='Multinomial (area = %0.4f)' % auc,color='red')\n","plt.legend()\n","\n","plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve - Naive\")"],"metadata":{"id":"OvUyA0IDZ-8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wSZd088zaDkH"},"source":["Naive Bayes has a complexity hyperparameter that is typically tuned - the smoothing value **`alpha`**.  You can try to see whether tuning `alpha` helps improve on the results above. (Try values of `alpha < 1` - the default)\n","\n","\n","Also, there are other versions of naive Bayes, for instance  **Gaussian Naive Bayes (GNB):** can be used when we have other numeric features that we can use in the predictive model (like, say, the age of the tweeter).  Sometimes GNB and Bernoulli NB are combined when one has features of mixed types.  \n","\n","\n","**Practice at home**:\n","- Create a tweet of your own that is about Flight Attendant Complaints and calculate what the model thinks the probability is of it being about a Flight Attendant problem.  \n","- Try some of these other models yourself, or maybe try a regularized (Lasso, Ridge) regression model.\n","- Or, see which type of complaint (Flight Attendant, Luggage, Bad flight) is the easiest to detect.  \n","- Cluster the data (Topic Modelling) without using the label and see if you come up with other interesting clusters\n","\n"]}]}