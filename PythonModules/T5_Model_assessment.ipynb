{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kFqT8W4fJvCHDF-HLm-eWCmoVLbkbSXq","timestamp":1728401894352}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DFaN0JqULvNI"},"source":["# Model Assessment\n","\n","\n","**Fall 2024 - Instructor:  Chris Volinsky**\n","\n","**Teaching Assistants: Aditya Deshpande, Stuti Mishra**\n","\n","**Original Notebooks courtesy of Prof. Foster Provost and Rubing Li**\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"7RO4OlvMLvNN"},"source":["Import all of the packages we will need."]},{"cell_type":"code","metadata":{"id":"PtodL8PHLvNR","scrolled":false},"source":["# Import the libraries we will be using\n","import numpy as np\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","\n","\n","import matplotlib.pylab as plt\n","%matplotlib inline\n","# plt.rcParams['figure.figsize'] = 15, 12"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Orw_IlwULvNZ"},"source":["### Data Input, EDA, and Data Cleaning\n","We're going to use a mail response data set from a real direct marketing campaign.  [You can download the data here](https://drive.google.com/uc?export=download&id=1deEx-Ey37F7qznPlIqmaAjjkmkvBtV28).  Each record represents an individual who was targeted with a direct marketing offer.  The offer was a solicitation to make a charitable donation.\n","\n","The columns (features) are:\n","\n","```\n","income       household income\n","Firstdate    data assoc. with the first gift by this individual\n","Lastdate     data associated with the most recent gift\n","Amount       average amount by this individual over all periods (incl. zeros)\n","rfaf2        frequency code\n","rfaa2        donation amount code\n","pepstrfl     flag indicating a star donator\n","glast        amount of last gift\n","gavr         amount of average gift\n","```\n","\n","The target variables is `class` and is equal to one if they gave in this campaign and zero otherwise."]},{"cell_type":"code","source":["# read in csv file from my computer\n","\n","from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"id":"7iUjiU-zngVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('DirectMarketing.csv') # replace 'your_file.csv' with your file name\n","# Now you can work with the dataframe df\n"],"metadata":{"id":"lrF0ZHdRno8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTxcwq11LvNd","scrolled":false},"source":["df.info()\n","df.describe().round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head() # note the cateogricals, that dont show up in .describe"],"metadata":{"id":"OG7q4R1hrhzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot histograms of all features\n","\n","df.hist(bins=50, figsize=(20, 15))\n","plt.show()"],"metadata":{"id":"6aKe_iiao3r4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to do some cleaning here...\n","\n","- There are no missing values, but there are 4 cases where Firstdate is zero.  We can remove those.\n","\n","- We have two heavily skewed features - `gavr` and `glast` - which we might want to consider transformations.  We will be fitting a regression model, so it is best not to have skewed features.\n","\n","- our target is unbalanced (5% class == 1), we might consider balancing it, but for now, no.\n","\n","- need to create dummies for our categorical features (rfaa2 and pepstrfl), but there are others that are also categorical but not coded that way (income and rfaf2)"],"metadata":{"id":"IkFAZmOAqTQ7"}},{"cell_type":"code","source":["# lets look closer at our very skewed features\n","#plot histograms of the log gavr and glast next to each other\n","\n","plt.figure(figsize=(5,3))\n","plt.hist(np.log(df.gavr+1), bins=50,ec='black')\n","plt.title('Histogram of log(gavr)')\n","plt.show()\n","plt.figure(figsize=(5,3))\n","plt.hist(np.log(df.glast+1), bins=50,ec='black')\n","plt.title('Histogram of log(glast)')\n","plt.show()"],"metadata":{"id":"XLbX3RYVq5E1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove cases where Firstdate == 0 using .loc\n","df = df.loc[df.Firstdate != 0]\n","\n","# replace gavr and glast with log versions of same features using .loc\n","df_clean = df\n","df_clean['gavr'] = np.log(df.gavr+1)\n","df_clean['glast'] = np.log(df.glast+1)\n","\n"],"metadata":{"id":"pDeAZNS5sQ7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDzQ2Ax2LvNm"},"source":["We want to create dummies for all of our categorical features.  But two of them (Income and rfaf2) are not yet recognized as categorical.  So we need to change them, and then create the dummies, for them and the other features `rfaa2` and `pepstrfl`.\n","\n","Use `drop_first=True` to create k-1 dummies as discussed in class."]},{"cell_type":"code","source":["income_cat = pd.Categorical(df['Income'], categories=[0,1,2,3,4,5,6,7])\n","df_clean['Income'] = income_cat\n","\n","rfaf2_cat = pd.Categorical(df['rfaf2'], categories=[1,2,3,4])\n","df_clean['rfaf2'] = rfaf2_cat\n","\n","df_clean = pd.get_dummies(df_clean, columns=['rfaa2', 'pepstrfl','Income','rfaf2'],drop_first=True)\n","df_clean.head()\n"],"metadata":{"id":"5mQCO3cgvHhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Think about the date features Firstdate and Lastdate...are these informative?  How might we make them more interesting?\n","\n","Maybe let:\n","\n","*tenure* = Lastdate-Firstdate\n","\n","and\n","\n","*recency* = Today-Lastdate\n","\n","(we dont know the date the data was collected or what these numbers mean - so lets just use the most recent date (max(lastdate)) as today"],"metadata":{"id":"aQmPzaI2yGv5"}},{"cell_type":"code","source":["# Create a new feature 'tenure'\n","df_clean['tenure'] = df_clean['Lastdate'] - df_clean['Firstdate']\n","\n","# maybe check to see this is always greater than zero?\n","df_clean['tenure'].min()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTSI2mu5yGRa","executionInfo":{"status":"ok","timestamp":1728419084922,"user_tz":240,"elapsed":204,"user":{"displayName":"Christopher Volinsky","userId":"09256355646661007650"}},"outputId":"6ed8fa3a-8f39-41d3-9055-d73240f89750"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["# assign \"today\" as the max value of Lastdate, and calculate \"recency\" as the difference between today and Lastdate\n","\n","today = df_clean['Lastdate'].max()\n","df_clean['recency'] = today - df_clean['Lastdate']\n","\n","# remove Firstdate and Lastdate\n","df_clean = df_clean.drop(['Firstdate', 'Lastdate'], axis=1)\n"],"metadata":{"id":"GINhj8lGykI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_clean.head()"],"metadata":{"id":"gXfok1OQyhbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_Yw7qSwLvNy"},"source":["### Build the model\n","\n","We are going to build a model - specifically Logistic Regression (a type of classification model - NOT a regression model ðŸ¤¯).   But the model is not useful until we apply *decision logic*, specfically defining the threshold that we will use to determine the action we will take."]},{"cell_type":"code","metadata":{"id":"t9Z6nZYGLvN2","scrolled":false},"source":["# Split our data into training and test sets\n","# remove the features we dont need anymore from our X features\n","X = df_clean.drop(['class'], axis=1)\n","Y = df_clean['class']\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n","\n","# Make and fit a model on the training data\n","model_lr = LogisticRegression(C=100000,solver = 'liblinear')\n","  # C is a regularlization parameter, will discuss in next chapter\n","  # liblinear is a good general purpose solver (algorithm)\n","model_lr.fit(X_train, Y_train)\n","\n","# Get probabilities of being a donor\n","probabilities = model_lr.predict_proba(X_test)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjrfsirxLvOA"},"source":["Use the default threshold of 50% to decide Y vs N.\n","\n","(An individual below this threshold will get a prediction of \"0\" and someone above this will get a prediction of \"1\")"]},{"cell_type":"code","source":["#plot histogram of probabilities\n","plt.hist(probabilities, bins=50,ec='black')\n","plt.show()"],"metadata":{"id":"qHFc1kfo0Ozu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What do you notice about the probabilities?\n","\n","Here is an example where using a 0.5 threshold does not make sense!  \n","\n","Pick a sensible threshold and use it to define your predicted probabilities"],"metadata":{"id":"Jk84jEk20io-"}},{"cell_type":"code","source":["thresh = ###\n","\n","pred_giver = probabilities > thresh\n","\n","# how many predicted givers are there?\n","print(\"Total predicted to give: \",pred_giver.sum())\n","print(\"% predicted to give: \", pred_giver.mean().round(3))\n","\n","#probabilities > = 0/05).value_counts()"],"metadata":{"id":"8Xtvl_g21X_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluating the Model with a Confusion Matrix\n","\n","Remember that the confusion matrix is a function of the model (Logistic Regression) but also the decision logic (threshold)  that we set.\n"],"metadata":{"id":"dG8QG7hZHSa2"}},{"cell_type":"code","metadata":{"id":"_LampQbELvOD","scrolled":false},"source":["conf_mat = pd.DataFrame(metrics.confusion_matrix(Y_test, pred_giver,labels=[1,0]).T)\n","# Take the transpose (.T) of the confusion matrix is not necessary,\n","# but it makes it so that the conf mat is the same orientation as in our slides\n","conf_mat.columns=['Actual1', 'Actual0']\n","conf_mat.index=['Pred1', 'Pred0']\n","conf_mat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Can you calculate the metrics we have learned?:\n","- accuracy\n","- precision\n","- recall\n","- F1\n","\n","I'm sure you can!!  ðŸ§ \n","\n","But Python can do it too..."],"metadata":{"id":"kutZY6ZxJnfo"}},{"cell_type":"code","source":["\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n","\n","precision = precision_score(Y_test, pred_giver)\n","recall = recall_score(Y_test, pred_giver)\n","accuracy = accuracy_score(Y_test, pred_giver)\n","f1 = f1_score(Y_test, pred_giver)\n","\n","\n","print(\"Precision:\", round(precision,3))\n","print(\"Recall:\", round(recall,3))\n","print(\"Accuracy:\", round(accuracy,3))\n","print(\"F1 Score:\", round(f1,3))\n"],"metadata":{"id":"SPEWRWcuImiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Also: What percentage of your customer base are you marketing to with this threshold???**\n","\n"],"metadata":{"id":"cGkSiAbvjgVk"}},{"cell_type":"markdown","metadata":{"id":"XT8Fl3iHLvOZ"},"source":["***\n","Is this good performance?\n","\n","How can we tell?\n","\n","Is your threshold the right threshold?  How would we determine that?"]},{"cell_type":"markdown","metadata":{"id":"ymJpg0gGLvOc"},"source":["## ROC Curves and AUC for comparing models\n","\n","Remember that the Area under the ROC curve (AUC) is a single value to compare models across all thresholds\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hN0MKedJLvOm"},"source":["\n","Using several classification models, we will plot  *ROC curves* and overlay them to see which has the best performance.\n"]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","models = {\n","    'Logistic Regression': LogisticRegression(C=10000, solver='liblinear'),\n","    'Decision Tree': DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=500, random_state=42),\n","    'Naive Bayes': GaussianNB(),\n","}\n","\n","colors = {\n","    'Logistic Regression': 'blue',\n","    'Decision Tree': 'orange',\n","    'Naive Bayes': 'red',\n","}\n","\n","# Dictionary to hold ROC data\n","roc_data = {}\n","\n","for name, model in models.items():\n","    model.fit(X_train, Y_train)\n","    Y_pred_proba = model.predict_proba(X_test)[:,1]\n","    fpr, tpr, _ = roc_curve(Y_test, Y_pred_proba)\n","    roc_auc = metrics.roc_auc_score(Y_test, Y_pred_proba)\n","    roc_data[name] = (fpr, tpr, roc_auc)\n","\n","# Plot ROC curves\n","plt.figure(figsize=(10, 8))\n","for name, (fpr, tpr, roc_auc) in roc_data.items():\n","    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.3f})',color=colors[name])\n","\n","plt.plot([0, 1], [0, 1], 'k--', label='Random chance')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curves for Various Models')\n","plt.legend(loc=\"lower right\")\n","plt.show()\n"],"metadata":{"id":"-0wQKYdG-rEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Who is the winner?  \n","Lets take a look at other curves..."],"metadata":{"id":"yoa4XGEhN6g9"}},{"cell_type":"markdown","metadata":{"id":"dN3wVwP2LvOx"},"source":["## Cumulative response and lift curves\n","\n","ROC is a useful curve to compare models in many situations.  But sometimes this interpretation can be confusing to stakeholders.  \n","\n"," In some applications the **cumulative response curve** and its associated **lift curve** can be more useful.  Specifically in cases - like targeted advertising - where you will be taking action on a small percentage of your customer base, the lift curve has an appealing interpretation : \"How much better does my model do than random chance\"?   The lift curve allows comments like **our best model improves our targeting by 4.5x**\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Unfortunately, Python does not have a built in model to calculate these curves, so we will provide some user-built functions ..."],"metadata":{"id":"wXNcarsMU9zM"}},{"cell_type":"code","metadata":{"id":"OBn8PB3qLvOz","scrolled":false},"source":["def build_cumulative_curve(model, scale=100):\n","    # Fit model\n","    model.fit(X_train, Y_train)\n","\n","    # Get the probability of Y_test records being = 1\n","    Y_test_probs = model.predict_proba(X_test)[:, 1]\n","\n","    # Sort these probabilities and the true value in descending order of probability\n","    order = np.argsort(Y_test_probs)[::-1]\n","    Y_test_probs_sorted = Y_test_probs[order]\n","    Y_test_sorted = np.array(Y_test)[order]\n","\n","    # Build the cumulative response curve\n","    x_cumulative = np.arange(len(Y_test_probs_sorted)) + 1\n","    y_cumulative = np.cumsum(Y_test_sorted)\n","\n","    # Rescale\n","    x_cumulative = np.array(x_cumulative)/float(x_cumulative.max()) * scale\n","    y_cumulative = np.array(y_cumulative)/float(y_cumulative.max()) * scale\n","\n","    return x_cumulative, y_cumulative\n","\n","def plot_cumulative_curve(models):\n","    # Plot curve for each model\n","    for key in models:\n","        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n","        plt.plot(x_cumulative, y_cumulative, label=key)\n","    # Plot other details\n","    plt.plot([0,100], [0,100], 'k--', label=\"Random\")\n","    plt.xlabel(\"Percentage of test instances targeted (decreasing score)\")\n","    plt.ylabel(\"Percentage of population positives targeted (Recall)\")\n","    plt.title(\"Cumulative response curve\")\n","    plt.legend()\n","\n","plot_cumulative_curve(models)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOJBGFUdLvO8"},"source":["We can also plot a **lift curve** in this scenario.  What if we specfically only want to market to the top 1% of our customer base?  Which is best?"]},{"cell_type":"code","metadata":{"id":"W0RQHeENLvO_","scrolled":false},"source":["def plot_lift_curve(models):\n","    # Plot curve for each model\n","    for key in models:\n","        x_cumulative, y_cumulative = build_cumulative_curve(models[key])\n","        plt.plot(x_cumulative, y_cumulative/x_cumulative, label=key)\n","    # Plot other details\n","    plt.plot([0,100], [1,1], 'k--', label=\"Random\")\n","    plt.xlabel(\"Percentage of test instances (decreasing score)\")\n","    plt.ylabel(\"Lift (times)\")\n","    plt.title(\"Lift curve\")\n","    plt.legend()\n","    plt.grid()\n","    plt.xlim(0, 100)  # Set x-axis limits to 0-20\n","\n","plot_lift_curve(models)\n","\n","## NOTE, might want to focus on right side of plot: plt.xlim(0, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iens5XJBLvPb"},"source":["What if we want to understand not just lift, but how much benefit we are going to receive from a certain investment in targeting?  We can plot a profit curve.\n"]},{"cell_type":"markdown","metadata":{"id":"UmBnUSrGLvPc"},"source":["## Profit curves\n","Let's say that each offer costs \\$1 to make and market, and each accepted offer earns \\$18, for a profit of $17. The cost matrix would be:\n"]},{"cell_type":"code","metadata":{"id":"d83AHt5ILvPe","scrolled":false},"source":["unit_cost = 1\n","unit_revenue = 18\n","\n","cost_matrix = pd.DataFrame([[unit_revenue - unit_cost, - unit_cost], [0, 0]], columns=['Actual1', 'Actual0'], index=['Pred1', 'Pred0'])\n","print (\"Cost matrix\")\n","print (cost_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aa9L3OiGLvPm"},"source":["Remember that we examined different targeting thresholds (aka \"decision thresholds\"):"]},{"cell_type":"code","metadata":{"id":"qXChRdV7LvPo","scrolled":false,"executionInfo":{"status":"ok","timestamp":1728432927971,"user_tz":240,"elapsed":178,"user":{"displayName":"Christopher Volinsky","userId":"09256355646661007650"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df4758bc-f1ac-48e9-f14c-418b21099027"},"source":["print (\"Confusion matrix with threshold =\",thresh,\"to send marketing:\\n\")\n","print (conf_mat)\n","print (\"\\n\")\n","# print (\"Confusion matrix with threshold = 5% to predict labels\")\n","# print (confusion_matrix_5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion matrix with threshold = 0.1 to send marketing:\n","\n","       Actual1  Actual0\n","Pred1      156     1236\n","Pred0     1790    35173\n","\n","\n"]}]},{"cell_type":"code","source":["#multiply the confusion matrix by the cost matrix, element wise, and add up the values\n","\n","import numpy as np\n","profit = np.sum(np.multiply(conf_mat.values, cost_matrix.values))\n","print(\"Profit: $\",profit)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jstkx07jaXp9","executionInfo":{"status":"ok","timestamp":1728432930713,"user_tz":240,"elapsed":190,"user":{"displayName":"Christopher Volinsky","userId":"09256355646661007650"}},"outputId":"30e74933-f922-4289-a951-2a45541054af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Profit: $ 1416\n"]}]},{"cell_type":"markdown","metadata":{"id":"6t3JqmMRLvP1"},"source":["Now we can plot the profit curve for the two models.\n","\n","(Note: to calculate the profit curve, you could simply cycle through all of the possible thresholds(using a `for` loop), and store the profit at each threshold, then plot it.\n","\n","This version is slightly more efficient."]},{"cell_type":"code","source":["## METHOD 1 - cycle through all of the possible thresholds(using a for loop), and store the profit at each threshold, then plot it.\n","## note this is just for one model.\n","\n","\n","model=LogisticRegression(C=10000, solver='liblinear')\n","model.fit(X_train, Y_train)\n","\n","## reorder your probabilities (and associated Y_test values) in order of sorted probabilities\n","probs = model.predict_proba(X_test)[:, 1]\n","order = np.argsort(probs)[::-1]\n","probs_sort = probs[order]\n","Y_test_sort = np.array(Y_test)[order]\n","\n","## cycle through the probability thresholds from highest to lowest\n","## at each stage, calculate number marketed to, and the number customers donating\n","xvec = []\n","yvec = []\n","tot_obs = len(Y_test)\n","tot_pos = Y_test.sum()\n","max_profit = 0\n","for prob in probs_sort[:tot_obs]:\n","  pred_pos = probs_sort >= prob\n","  Y_test[pred_pos].mean()\n","  num_donated_customers = Y_test_sort[pred_pos].sum()\n","  expense = unit_cost * pred_pos.sum()\n","  revenue = unit_revenue * num_donated_customers\n","  profit = revenue - expense\n","  xval = pred_pos.sum()/tot_obs\n","  yval = profit\n","  if profit > max_profit:\n","    max_profit = profit\n","    max_profit_targeted = xval\n","  xvec.append(xval)\n","  yvec.append(yval)\n"," # print(prob, pred_pos.sum(), num_donated_customers, expense, revenue, profit)\n","\n","print(\"Max Profit = %.0f\" % max_profit,\" with %.3f\" %max_profit_targeted,\" % targeted\\n\")\n","# format the print statement with 3 decimal place\n","\n","\n","# plot xvec vs yvec\n","plt.plot(xvec,yvec)\n","plt.plot(max_profit_targeted, max_profit, 'ko')\n","plt.xlabel(\"Percentage of users targeted\")\n","plt.ylabel(\"Profit\")\n","plt.title(\"Profit curve\")\n","plt.show()\n"],"metadata":{"id":"WH8U5tWECKQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHjoTZUlLvP3"},"source":["# METHOD 2 - uses our cumulative response curves from above.\n","\n","unit_cost = -cost_matrix['Actual0']['Pred1']\n","unit_revenue = cost_matrix['Actual1']['Pred1'] + unit_cost\n","\n","def plot_profit_curve(models):\n","    # Plot curve for each model\n","    total_obs = len(Y_test)\n","    total_pos = Y_test.sum()\n","    for key in models:\n","        x_cumulative, y_cumulative = build_cumulative_curve(models[key], scale=1)\n","        # x_cumulative is cumulative vector of what percentage of population I am targeting\n","        # y_cumulative is cumulative vector of % of positives among those I am targeting\n","        pos_profit = unit_revenue * y_cumulative * total_pos\n","        neg_profit = unit_cost * x_cumulative * total_obs\n","        profits = pos_profit - neg_profit\n","        max_profit = profits.max().round(2)\n","        max_profit_targeted = (100 * x_cumulative[profits.argmax(axis=0)]).round(1)\n","        plt.plot(x_cumulative * 100, profits, label=key)\n","        print(\"max profit is\",max_profit,\"with\",max_profit_targeted,\"% users targeted\")\n","        plt.plot(max_profit_targeted, max_profit, 'ko')\n","    # Plot other details\n","    plt.xlabel(\"Percentage of users targeted\")\n","    plt.ylabel(\"Profit\")\n","    plt.title(\"Profit curve\")\n","    plt.legend()\n","\n","plt.figure(figsize=(20,12))\n","plot_profit_curve(models)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwLaH1vWLvP9"},"source":["Which one do you think we should choose? Why?"]},{"cell_type":"markdown","source":["## Self-Directed Exercise\n","\n","Run notebook again by changing things:\n","- change cost matrix\n","- add new models\n","- re-balance data at beginning by sampling class=0 to create a new set with 50% 1s and 0s.  This might allow you to fit more complex models like k-NN or SVM without it taking so long.\n","\n"],"metadata":{"id":"Jbalm9dS0cqY"}}]}