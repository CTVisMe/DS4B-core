{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1k24jrNj4Wf43E4pudYi59QFieBRLpmEN","timestamp":1729022288376}],"authorship_tag":"ABX9TyNifFGeGtFGGEY6RuY2V9Ru"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this example we are going to fit Regression Trees to a data set that measures prices for a collection of Toyota Corollas.\n","\n","Additionally we will learn how to do a *grid search* over many different parameters, basically learning how to optimize multiple parameters at once.\n","\n","We will start by importing a new Python library called `dmba` (this is the Python library for data sets in the Shmueli book)\n","and then reading \"ToyotaCorolla.csv\" from that package:"],"metadata":{"id":"jhgTiPV7x63H"}},{"cell_type":"code","source":["!pip install dmba"],"metadata":{"id":"ko1Xagv-Ucbm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dmba import load_data\n","\n","# Load the ToyotaCorolla dataset\n","toyota_df = load_data('ToyotaCorolla.csv')\n","\n","\n","# Rename some of the annoyingly named features\n","toyota_df = toyota_df.rename(columns={'Age_08_04': 'Age', 'Quarterly_Tax': 'Tax'})\n","\n","\n","# Display the first few rows\n","print(toyota_df.head())\n","print(toyota_df.info())"],"metadata":{"id":"aFZTepeazDBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["toyota_df.describe()"],"metadata":{"id":"NK9_qIL51V07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Regression Trees\n","Fitting Regression trees to Toyota Corolla data\n","Shmueli Chapter 9.6\n","(data introduced in chapter 6).\n","\n","Lets install some other important packages\n","\n"],"metadata":{"id":"-rnCyiHFeK2d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDqM8mKiQOS9"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pylab as plt\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from dmba import regressionSummary, classificationSummary\n"]},{"cell_type":"code","source":["\n","predictors = ['Age', 'KM', 'Fuel_Type', 'HP', 'Met_Color', 'Automatic', 'CC',\n","              'Doors', 'Tax', 'Weight']\n","outcome = 'Price'\n","\n","X = pd.get_dummies(toyota_df[predictors], drop_first=True)\n","y = toyota_df[outcome]\n","\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)"],"metadata":{"id":"gAWeBkQTx1Kg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit a regression tree of depth 3\n","regTree = DecisionTreeRegressor(max_depth=3, random_state=13, criterion=\"absolute_error\")\n","regTree.fit(train_X, train_y)\n","# plot it\n","plt.figure(figsize=(20,15))\n","plot_tree(regTree, feature_names=X.columns, filled=True, rounded=True)\n","plt.show()"],"metadata":{"id":"qgbKq0IP1FOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Grid Search\n","\n","Previously we have learned how to optimize a single parameter (like `max_depth`) while fitting a tree model.  However, there are several paramters you could choose from...many models have several parameters you can optimize for complexity control. How can we optimize across multiple parameters?  \n","\n","\n","**Grid Search** is one way to find optimized parameters across many different options.  In tree fitting we have many things we can vary...we can use `GridSearchCV()` as a way to optimize across all of them using cross validation.\n","\n","First you identify all of your parameters and the grid using `param_grid`..."],"metadata":{"id":"ob1IktFpdtDn"}},{"cell_type":"code","source":["# user grid search to find optimized tree\n","param_grid = {\n","    'max_depth': [1, 5, 10, 15, 20, 25],\n","    'min_samples_split': [2, 5, 10, 15, 20, 25, 30],\n","    'max_features': [1,3,5,7,9]\n","}\n","\n","gridSearch = GridSearchCV(DecisionTreeRegressor(random_state=13), param_grid, cv=5, n_jobs=-1)\n","gridSearch.fit(train_X, train_y)\n","print('First level optimal parameters: ', gridSearch.best_params_)\n"],"metadata":{"id":"DesqlDOXd43h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","## now that we are narrowing in on the best options, lets refine some more:\n","\n","param_grid = {\n","    'max_depth': [6,7,8,9,10,11,12,13,14],\n","    'min_samples_split': [6,7,8,9,10,11,12,13,14,15],\n","    'max_features': [4,5,6]\n","}\n","gridSearch = GridSearchCV(DecisionTreeRegressor(random_state=13,criterion=\"absolute_error\"), param_grid, cv=5, n_jobs=-1)\n","gridSearch.fit(train_X, train_y)\n","print('Improved parameters: ', gridSearch.best_params_)\n","\n","regTree = gridSearch.best_estimator_\n"],"metadata":{"id":"Z4Z24H-Ty3dE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"summary on holdout data\")\n","regressionSummary(test_y, regTree.predict(test_X))"],"metadata":{"id":"H7_XFuF95Aat"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the best tree\n","plt.figure(figsize=(40,35))\n","plot_tree(regTree, feature_names=X.columns, filled=True, rounded=True)\n","plt.show()"],"metadata":{"id":"rie2ql7-5Qmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## and you can still plot feature importances\n","\n","\n","scores = regTree.feature_importances_\n","scores\n","sorted_pairs = sorted(zip(scores, X.columns), reverse=True)\n","sorted_scores, sorted_names = zip(*sorted_pairs)\n","\n","# plot it\n","\n","plt.bar(sorted_names, sorted_scores, color='skyblue')  # You can change the color\n","\n","# Add title and labels\n","plt.title('Scores by Name')\n","plt.xlabel('Name')\n","plt.ylabel('Score')\n","\n","# Display the plot\n","plt.xticks(rotation=45)  # Rotate names to prevent overlap\n","plt.show()\n"],"metadata":{"id":"Q-uqsU1nCZrQ"},"execution_count":null,"outputs":[]}]}